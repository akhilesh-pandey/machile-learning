{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import errno\n",
    "import torch\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_int(b):\n",
    "    return int(codecs.encode(b, 'hex'), 16)\n",
    "\n",
    "\n",
    "def parse_byte(b):\n",
    "    if isinstance(b, str):\n",
    "        return ord(b)\n",
    "    return b\n",
    "\n",
    "\n",
    "def read_label_file(path):\n",
    "    print(\"label Path\", path)\n",
    "    with open(path, 'rb') as f:\n",
    "        data = f.read()\n",
    "        assert get_int(data[:4]) == 2049\n",
    "        length = get_int(data[4:8])\n",
    "        labels = [parse_byte(b) for b in data[8:]]\n",
    "        assert len(labels) == length\n",
    "        print(\"Reading labels complete\")\n",
    "        return torch.LongTensor(labels)\n",
    "\n",
    "\n",
    "def read_image_file(path):\n",
    "    print(\"data Path\", path)\n",
    "    with open(path, 'rb') as f:\n",
    "        data = f.read()\n",
    "        assert get_int(data[:4]) == 2051\n",
    "        length = get_int(data[4:8])\n",
    "        num_rows = get_int(data[8:12])\n",
    "        num_cols = get_int(data[12:16])\n",
    "        images = []\n",
    "        idx = 16\n",
    "        for l in range(length):\n",
    "            img = []\n",
    "            images.append(img)\n",
    "            for r in range(num_rows):\n",
    "                row = []\n",
    "                img.append(row)\n",
    "                for c in range(num_cols):\n",
    "                    row.append(parse_byte(data[idx]))\n",
    "                    idx += 1\n",
    "        assert len(images) == length\n",
    "        print(\"Reading data complete\")\n",
    "        return torch.ByteTensor(images).view(-1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I assume that the dataset is availabe in folder called 'raw'\n",
    "\n",
    "Give full path to the folder. In my system the data is given in the path:'/home/akhilz/academics/2sem/pytorch/raw/'\n",
    "Download data from github and after extracting put it in the folder 'raw' and change the path in below two cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data Path /home/akhilz/academics/2sem/pytorch/raw/train-images-idx3-ubyte\n",
      "Reading data complete\n",
      "label Path /home/akhilz/academics/2sem/pytorch/raw/train-labels-idx1-ubyte\n",
      "Reading labels complete\n"
     ]
    }
   ],
   "source": [
    "training_set= (read_image_file('/home/akhilz/academics/2sem/pytorch/raw/train-images-idx3-ubyte'),\n",
    "                read_label_file('/home/akhilz/academics/2sem/pytorch/raw/train-labels-idx1-ubyte'\n",
    "                                ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data Path /home/akhilz/academics/2sem/pytorch/raw/test-images-idx3-ubyte\n",
      "Reading data complete\n",
      "label Path /home/akhilz/academics/2sem/pytorch/raw/test-labels-idx1-ubyte\n",
      "Reading labels complete\n"
     ]
    }
   ],
   "source": [
    "test_set= (read_image_file('/home/akhilz/academics/2sem/pytorch/raw/test-images-idx3-ubyte'),\n",
    "          read_label_file('/home/akhilz/academics/2sem/pytorch/raw/test-labels-idx1-ubyte'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/processed/training.pt','wb') as f:\n",
    "    torch.save(training_set,f)\n",
    "\n",
    "with open('./data/processed/test.pt','wb') as f:\n",
    "    torch.save(test_set,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class fashion(data.Dataset):\n",
    "    raw_folder= 'raw'\n",
    "    processed_folder= 'processed'\n",
    "    training_file= 'training.pt'\n",
    "    test_file= 'test.pt'\n",
    "    \n",
    "    def __init__(self, root, train= True, transform= None, target_transform= None, download= False):\n",
    "        # if root= '~/folder_name/' os.path.expanduser(root) converts to /home/folder_name\n",
    "        self.root= os.path.expanduser(root) \n",
    "        self.transform= transform\n",
    "        self.target_transform= target_transform\n",
    "        self.train= train\n",
    "    \n",
    "        if not self.__check_exists():\n",
    "            raise RuntimeError('Dataset not found.')\n",
    "    \n",
    "        if self.train:\n",
    "            self.train_data, self.train_labels = torch.load(os.path.join(root, self.processed_folder, self.training_file))\n",
    "        else:\n",
    "            self.test_data, self.test_labels = torch.load(os.path.join(root, self.processed_folder, self.test_file))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            img, target= self.train_data[index], self.train_labels[index]\n",
    "        else:\n",
    "            img, target= self.test_data[index], self.test_labels[index]\n",
    "        \n",
    "        #read about PIL\n",
    "        #It is done to make this module consistent with all other datasets say RGB etc.\n",
    "        img= Image.fromarray(img.numpy(), mode='L')\n",
    "        \n",
    "        if self.transform:\n",
    "            img= self.transform(img)\n",
    "        \n",
    "        if self.target_transform:\n",
    "            target= self.target_transform(target)\n",
    "        \n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.train_data)\n",
    "        return len(self.test_data)\n",
    "    \n",
    "    def __check_exists(self):\n",
    "        print(os.path.join(self.root, self.processed_folder, self.test_file))\n",
    "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file)) and\\\n",
    "                os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
    "        \n",
    "    def get_int(self,b):\n",
    "        return int(codecs.encode(b, 'hex'), 16)\n",
    "\n",
    "\n",
    "    def parse_byte(self,b):\n",
    "        if isinstance(b, str):\n",
    "            return ord(b)\n",
    "        return b\n",
    "\n",
    "\n",
    "    def read_label_file(self,path):\n",
    "        with open(path, 'rb') as f:\n",
    "            data = f.read()\n",
    "            assert self.get_int(data[:4]) == 2049\n",
    "            length = self.get_int(data[4:8])\n",
    "            labels = [self.parse_byte(b) for b in data[8:]]\n",
    "            assert len(labels) == length\n",
    "            return torch.LongTensor(labels)\n",
    "    \n",
    "    def read_image_file(self,path):\n",
    "        with open(path, 'rb') as f:\n",
    "            data = f.read()\n",
    "            assert self.get_int(data[:4]) == 2051\n",
    "            length = self.get_int(data[4:8])\n",
    "            num_rows = self.get_int(data[8:12])\n",
    "            num_cols = self.get_int(data[12:16])\n",
    "            images = []\n",
    "            idx = 16\n",
    "            for l in range(length):\n",
    "                img = []\n",
    "                images.append(img)\n",
    "                for r in range(num_rows):\n",
    "                    row = []\n",
    "                    img.append(row)\n",
    "                    for c in range(num_cols):\n",
    "                        row.append(self.parse_byte(data[idx]))\n",
    "                        idx += 1\n",
    "            assert len(images) == length\n",
    "            return torch.ByteTensor(images).view(-1, 28, 28)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1\n",
    "Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalize= transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
    "                               std= [x/255.0 for x in [63.0, 62.1, 66.7]])\n",
    "transform= transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.137,),(0.3081,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/akhilz/academics/2sem/pytorch/processed/test.pt\n",
      "/home/akhilz/academics/2sem/pytorch/processed/test.pt\n"
     ]
    }
   ],
   "source": [
    "train_dataset= fashion(root='/home/akhilz/academics/2sem/pytorch/',\n",
    "                       train= True, transform= transform)\n",
    "test_dataset= fashion(root='/home/akhilz/academics/2sem/pytorch/', \n",
    "                      train= False, transform= transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2\n",
    "\n",
    "Making dataset iterable\n",
    "\n",
    "One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE.\n",
    "\n",
    "Batch_size: Total number of training examples present in a single batch.\n",
    "\n",
    "Iterations is the number of batches needed to complete one epoch.\n",
    "\n",
    "The number of batches is equal to number of iterations for one epoch.\n",
    "\n",
    "How to calculate batch_size,num_epochs, n_iters given dataset size?\n",
    "\n",
    "Example: We can divide the dataset of 2000 examples into batches of 500 then it will take 4 iterations to complete 1 epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size= 256\n",
    "n_iters= 4000\n",
    "num_epochs= n_iters /(len(train_dataset) / batch_size)\n",
    "num_epochs= int(num_epochs)\n",
    "\n",
    "train_loader= torch.utils.data.DataLoader(dataset= train_dataset,\n",
    "                                         batch_size= batch_size,\n",
    "                                         shuffle= False)\n",
    "test_loader=  torch.utils.data.DataLoader(dataset= test_dataset,\n",
    "                                         batch_size= batch_size,\n",
    "                                         shuffle= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3\n",
    "\n",
    "Creating a model (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        # Convolution 1\n",
    "        self.cnn1= nn.Conv2d(#input shape(1,28,28)\n",
    "            in_channels=1, #input height\n",
    "            out_channels= 16, #n_filters\n",
    "            kernel_size= 5, #filter size\n",
    "            stride= 1, #filter movement step\n",
    "            padding= 0) # if want same width and length of this image after con2d, padding=(kernel_size-1)/2 if stride=1\n",
    "            # output shape(16,24,24)\n",
    "        self.bc1= nn.BatchNorm2d(16)\n",
    "        self.relu1= nn.ReLU() #activation\n",
    "        \n",
    "        # MAx pool 1\n",
    "        self.maxpool1= nn.MaxPool2d(kernel_size= 2)  # choose max value in 2x2 area, output shape (16, 12, 12)\n",
    "        \n",
    "        \n",
    "        # Convolution 2\n",
    "        self.cnn2= nn.Conv2d( #input size (16,12,12)\n",
    "            in_channels= 16, \n",
    "            out_channels= 32, \n",
    "            kernel_size= 5, \n",
    "            stride= 1, \n",
    "            padding= 0) #output(32,8,8)\n",
    "        self.bc2= nn.BatchNorm2d(32)\n",
    "        self.relu2= nn.ReLU()\n",
    "        #MAx pool 2\n",
    "        self.maxpool2= nn.MaxPool2d(kernel_size= 2)#output size(32, 10, 10)\n",
    "        \n",
    "        self.dropout= nn.Dropout(p= 0.5)\n",
    "        \n",
    "        #Fully connected 1\n",
    "        self.fc1= nn.Linear(32*4*4,10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Convolution 1\n",
    "        out= self.cnn1(x)\n",
    "        out= self.bc1(out)\n",
    "        out= self.relu1(out)\n",
    "        out= self.maxpool1(out)\n",
    "        \n",
    "        #convolution 2\n",
    "        \n",
    "        out= self.cnn2(out)\n",
    "        out= self.bc2(out)\n",
    "        out= self.relu2(out)\n",
    "        out= self.maxpool2(out)\n",
    "        \n",
    "        #after maxpool2 the output size is(100, 32, 4, 4)\n",
    "        out= out.view(out.size(0), -1)\n",
    "        #now the output size is(100, 32*4*4)\n",
    "        \n",
    "        out= self.dropout(out)\n",
    "        \n",
    "        #apply fully connected layer(linear function)\n",
    "        out= self.fc1(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Instantiate model class       \n",
    "model= CNNModel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To print the architecure of your model just do the following. Check if it gives what you expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNModel (\n",
      "  (cnn1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (bc1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (relu1): ReLU ()\n",
      "  (maxpool1): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (cnn2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (bc2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (relu2): ReLU ()\n",
      "  (maxpool2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (dropout): Dropout (p = 0.5)\n",
      "  (fc1): Linear (512 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5\n",
    "\n",
    "Instantiate Loss class and Optimizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Instantiate Loss class\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "\n",
    "#Instantiate optimizer class\n",
    "learning_rate= 0.01\n",
    "optimizer= torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6\n",
    "\n",
    "We are half way done.  \n",
    "Now we are ready to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.3766932785511017. Accuracy: 85.95\n",
      "Iteration: 1000. Loss: 0.34714993834495544. Accuracy: 87.27\n",
      "Iteration: 1500. Loss: 0.3242199420928955. Accuracy: 87.54\n",
      "Iteration: 2000. Loss: 0.28273841738700867. Accuracy: 87.59\n",
      "Iteration: 2500. Loss: 0.24641162157058716. Accuracy: 87.92\n",
      "Iteration: 3000. Loss: 0.2886813282966614. Accuracy: 88.29\n",
      "Iteration: 3500. Loss: 0.3215324878692627. Accuracy: 88.06\n"
     ]
    }
   ],
   "source": [
    "iter= 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        images= Variable(images)\n",
    "        labels= Variable(labels)\n",
    "        \n",
    "        #clear gradient w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Forward pass to get output/logits\n",
    "        outputs= model(images)\n",
    "        \n",
    "        #Calculates the loss :  we are using softmax cross entropy loss\n",
    "        loss= criterion(outputs, labels)\n",
    "        \n",
    "        #Getting gradient w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        #update parametrs\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            #calculate accuracy\n",
    "            correct= 0\n",
    "            total= 0\n",
    "            \n",
    "            #Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                images= Variable(images)\n",
    "                \n",
    "                #Forward pass to get the output/logits\n",
    "                outputs= model(images)\n",
    "                \n",
    "                #Get prediction from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                #Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                correct += (predicted == labels).sum()\n",
    "                \n",
    "            accuracy= 100 * correct / total\n",
    "            \n",
    "            #print loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_model= True\n",
    "if save_model is True:\n",
    "    #save only parameters\n",
    "    torch.save(model.state_dict(),'fashion_model_trained.pk1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
