{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn.metrics import accuracy_score\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_cost(theta, n_classes, input_size, lambda_, data, labels):\n",
    "    \"\"\"\n",
    "    Compute the cost and derivative.\n",
    "    n_classes: the number of classes\n",
    "    input_size: the size N of the input vector\n",
    "    lambda_: weight decay parameter\n",
    "    data: the N x M input matrix, where each column data[:, i] corresponds to\n",
    "          a single test set\n",
    "    labels: an M x 1 matrix containing the labels corresponding for the input data\n",
    "    \"\"\"\n",
    "    # k stands for the number of classes\n",
    "    # n is the number of features and m is the number of samples\n",
    "    k = n_classes\n",
    "    n, m = data.shape\n",
    "    # Reshape theta\n",
    "    theta = theta.reshape((k, n))\n",
    "    # Probability with shape (k, m)\n",
    "    theta_data = theta.dot(data)\n",
    "    alpha = np.max(theta_data, axis=0)\n",
    "    theta_data -= alpha # Avoid numerical problem due to large values of exp(theta_data)\n",
    "    proba = np.exp(theta_data) / np.sum(np.exp(theta_data), axis=0)\n",
    "    # Matrix of indicator fuction with shape (k, m)\n",
    "    labels = np.reshape(labels,(m,))\n",
    "    indicator = scipy.sparse.csr_matrix((np.ones(m), (labels, np.arange(m))))\n",
    "    indicator = np.array(indicator.todense())\n",
    "    # Cost function\n",
    "    cost = -1.0/m * np.sum(indicator * np.log(proba)) + 0.5*lambda_*np.sum(theta*theta)\n",
    "    # Gradient matrix with shape (k, n)\n",
    "    grad = -1.0/m * (indicator - proba).dot(data.T) + lambda_*theta\n",
    "    # Unroll the gradient matrices into a vector\n",
    "    grad = grad.ravel()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a gradient descent method to obtain optimum parameter values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gdm(theta, n_classes, input_size, lambda_,data,labels):\n",
    "    #norm= np.linalg.norm(thetaOld - theta,ord= 2)\n",
    "    #print \"norm\", norm\n",
    "    thetaOld= theta*9999\n",
    "    errTol=0.001\n",
    "    while np.linalg.norm(thetaOld - theta,ord= 2) > errTol:\n",
    "        thetaOld= theta\n",
    "        theta= thetaOld - lambda_ * softmax_cost(theta, n_classes, input_size, lambda_, data, labels)\n",
    "        norm= np.linalg.norm(thetaOld - theta,ord=2)\n",
    "        #print \"norm\",norm,\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_train(input_size, n_classes, lambda_, input_data, labels, options={'maxiter': 400, 'disp': True}):\n",
    "    \"\"\"\n",
    "    Train a softmax model with the given parameters on the given data.\n",
    "    Returns optimal theta, a vector containing the trained parameters\n",
    "    for the model.\n",
    "    input_size: the size of an input vector x^(i)\n",
    "    n_classes: the number of classes\n",
    "    lambda_: weight decay parameter\n",
    "    input_data: an N by M matrix containing the input data, such that\n",
    "                input_data[:, c] is the c-th input\n",
    "    labels: M by 1 matrix containing the class labels for the\n",
    "            corresponding inputs. labels[c] is the class label for\n",
    "            the c-th input\n",
    "    options (optional): options\n",
    "      options['maxiter']: number of iterations to train for\n",
    "    \"\"\"\n",
    "    # initialize parameters\n",
    "    theta = 0.005 * np.random.randn(n_classes * input_size)\n",
    "    #J = lambda theta : softmax_cost(theta, n_classes, input_size, lambda_, input_data, labels)\n",
    "    # Find out the optimal theta\n",
    "    #results = scipy.optimize.minimize(J, theta, method='L-BFGS-B', jac=True, options=options)\n",
    "    #opt_theta = results['x']\n",
    "    opt_theta= gdm(theta, n_classes, input_size, lambda_, input_data, labels)\n",
    "    model = {'opt_theta': opt_theta, 'n_classes': n_classes, 'input_size': input_size}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_predict(model, data):\n",
    "    \"\"\"\n",
    "    model: model trained using softmax_train\n",
    "    data: the N x M input matrix, where each column data[:, i] corresponds to\n",
    "          a single test set\n",
    "    pred: the prediction array.\n",
    "    \"\"\"\n",
    "    theta = model['opt_theta'] # Optimal theta\n",
    "    k = model['n_classes']  # Number of classes\n",
    "    n = model['input_size'] # Input size (number of features)\n",
    "    # Reshape theta\n",
    "    theta = theta.reshape((k, n))\n",
    "    # Probability with shape (k, m)\n",
    "    theta_data = theta.dot(data)\n",
    "    alpha = np.max(theta_data, axis=0)\n",
    "    theta_data -= alpha # Avoid numerical problem due to large values of exp(theta_data)\n",
    "    proba = np.exp(theta_data) / np.sum(np.exp(theta_data), axis=0)\n",
    "    # Prediction values\n",
    "    pred = np.argmax(proba, axis=0)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload train data, train label, test data  and test label. I assume that we are working on mnist dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = load(tainDAta)\n",
    "y = load(testLabels)\n",
    "X_test = load(testData)\n",
    "y_test = load(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 28 * 28 # Size of input vector (MNIST images are 28x28)\n",
    "n_classes = 10       # Number of classes (MNIST images fall into 10 classes)\n",
    "lambda_ = 0.5 # Weight decay parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "options = {'maxiter': 100, 'disp': True}\n",
    "model = softmax_train(input_size, n_classes, lambda_, X, y, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = softmax_predict(model, X_test)\n",
    "y_test = y_test.tolist()\n",
    "pred = pred.tolist()\n",
    "print accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PERFORMANCE ANALYSIS\n",
    "\n",
    "|Error tolerance|learning rate|Accuracy|\n",
    "|---------------|-------------|--------|\n",
    "|     0.002     |   0.001     | 85     |\n",
    "|     0.002     |    0.05     | 87     |\n",
    "|     0.002     |    0.1      |  87.29 |\n",
    "|     0.001     |    0.5      |  86.79 |\n",
    "|     0.001     |    0.1      |  86.79 |\n",
    "\n",
    "From above analysis we can conclude that this model gives accuracy of 87.29 % with parameters set to error tolerance = 0.002 and step size = 0.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
